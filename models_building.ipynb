{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83a83341",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "\n",
    "TODO -> Try with Stratified datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d435b1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import poisson\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import xgboost as xgb\n",
    "xgb.set_config(verbosity=0)\n",
    "from sklearn.feature_selection import RFECV\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "29a92679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop first column of dataframe\n",
    "def drop_first_col(df):\n",
    "    return df.iloc[: , 1:]\n",
    "\n",
    "X_train = pd.read_csv(\"dataset\\X_train.csv\")\n",
    "X_test = pd.read_csv(\"dataset\\X_test.csv\")\n",
    "y_train = pd.read_csv(\"dataset\\y_train.csv\")\n",
    "y_test = pd.read_csv(\"dataset\\y_test.csv\")\n",
    "\n",
    "X_train = drop_first_col(X_train)\n",
    "X_test = drop_first_col(X_test)\n",
    "y_train = drop_first_col(y_train)\n",
    "y_test = drop_first_col(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c6482",
   "metadata": {},
   "source": [
    "## Test on single simple models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c80bba",
   "metadata": {},
   "source": [
    "### Single XGB Classifier (with no hyperparameters selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9c01f28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7037037037037037\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.45      0.56        11\n",
      "           1       0.70      0.88      0.78        16\n",
      "\n",
      "    accuracy                           0.70        27\n",
      "   macro avg       0.71      0.66      0.67        27\n",
      "weighted avg       0.71      0.70      0.69        27\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier(use_label_encoder =False)\n",
    "clf.fit(newX_train, y_train)\n",
    "y_predicted = clf.predict(newX_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_predicted))\n",
    "print(classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8885d3",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "98c7d574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6666666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.64      0.61        11\n",
      "           1       0.73      0.69      0.71        16\n",
      "\n",
      "    accuracy                           0.67        27\n",
      "   macro avg       0.66      0.66      0.66        27\n",
      "weighted avg       0.67      0.67      0.67        27\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marti\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(kernel='linear') \n",
    "  \n",
    "# fitting x samples and y classes \n",
    "clf.fit(X_train, y_train) \n",
    "y_predicted = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_predicted))\n",
    "print(classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df80d3c",
   "metadata": {},
   "source": [
    "### Single Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a09f5463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-44-22d5d04bd9f3>:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  model = RandomForestClassifier().fit(X_train,y_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5925925925925926\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.18      0.27        11\n",
      "           1       0.61      0.88      0.72        16\n",
      "\n",
      "    accuracy                           0.59        27\n",
      "   macro avg       0.55      0.53      0.49        27\n",
      "weighted avg       0.56      0.59      0.53        27\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate random forest algorithm for classification\n",
    "# define the model\n",
    "model = RandomForestClassifier().fit(X_train,y_train)\n",
    "y_predicted = model.predict(X_test)\n",
    "# report performance\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_predicted))\n",
    "print(classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f40b2a8",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c49ed00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marti\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\marti\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6296296296296297\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.55      0.55        11\n",
      "           1       0.69      0.69      0.69        16\n",
      "\n",
      "    accuracy                           0.63        27\n",
      "   macro avg       0.62      0.62      0.62        27\n",
      "weighted avg       0.63      0.63      0.63        27\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "y_predicted = model.predict(X_test)\n",
    "# report performance\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_predicted))\n",
    "print(classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4956197",
   "metadata": {},
   "source": [
    "## Features Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d95a1d",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination (RFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "abc872ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfe_xgb(X_train, y_train):\n",
    "    min_features_to_select = 1\n",
    "    \n",
    "    #run RFE on current train subset\n",
    "    classifier = xgb.XGBClassifier(use_label_encoder =False)\n",
    "    rfecv = RFECV(estimator=classifier,min_features_to_select=min_features_to_select,step=3,n_jobs=-1,scoring=\"r2\",cv=5)\n",
    "    rfecv.fit(X_train, y_train)\n",
    "    \n",
    "    #keep selected features + check RFE accuracy scores during running\n",
    "    newX_train = X_train[X_train.columns[rfecv.support_]]\n",
    "    newX_test = X_test[X_test.columns[rfecv.support_]]\n",
    "    print('We kept',newX_train.shape[1],'features out of the',X_train.shape[1])\n",
    "\n",
    "    plt.figure()\n",
    "    lines = []\n",
    "    for i in range(rfecv.grid_scores_.shape[1]):\n",
    "        lines.append(plt.plot(rfecv.grid_scores_[:,i]))\n",
    "\n",
    "    plt.xlabel(\"Number of features selected\")\n",
    "    plt.ylabel(\"Cross validation score (accuracy)\")\n",
    "    plt.legend(lines,labels = ['fold 1', 'fold 2', 'fold 3','fold 4', 'fold 5'],loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()\n",
    "    \n",
    "    return newX_train,newX_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67631c6e",
   "metadata": {},
   "source": [
    "### Test on single XGB classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cc922337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGB_class(X_train, X_test, y_train, y_test,learning_rate, n_estimators, max_depth,min_child_weight, gamma, subsample, colsample_bytree, simple=True):\n",
    "    \n",
    "    if simple:\n",
    "        clf = xgb.XGBClassifier(seed = 24, use_label_encoder =False)\n",
    "    else:\n",
    "        clf = xgb.XGBClassifier(learning_rate = learning_rate, n_estimators = int(n_estimators), max_depth = int(max_depth), \n",
    "                                min_child_weight = min_child_weight, gamma = gamma, subsample = subsample, \n",
    "                                colsample_bytree = colsample_bytree, seed = 24,eval_metric='mlogloss',use_label_encoder =False)\n",
    "\n",
    "    clf.fit(newX_train, y_train)\n",
    "    y_predicted = clf.predict(newX_test)\n",
    "\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_predicted))\n",
    "    print(classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5c5e9b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6666666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.36      0.47        11\n",
      "           1       0.67      0.88      0.76        16\n",
      "\n",
      "    accuracy                           0.67        27\n",
      "   macro avg       0.67      0.62      0.61        27\n",
      "weighted avg       0.67      0.67      0.64        27\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test rfe then simple XGB classifier\n",
    "newX_train,newX_test = rfe_xgb(X_train, y_train)\n",
    "XGB_class(newX_train,newX_test,y_train,y_test,0,0,0,0,0,0,0,simple=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17e3d52",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning XGBoost classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43353abf",
   "metadata": {},
   "source": [
    "### Using Evolutionary Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0c85ca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(723)\n",
    "np.random.seed(723)\n",
    "\n",
    "def initilialize_poplulation(numberOfParents):\n",
    "    learningRate = np.empty([numberOfParents, 1])\n",
    "    nEstimators = np.empty([numberOfParents, 1], dtype = np.uint8)\n",
    "    maxDepth = np.empty([numberOfParents, 1], dtype = np.uint8)\n",
    "    minChildWeight = np.empty([numberOfParents, 1])\n",
    "    gammaValue = np.empty([numberOfParents, 1])\n",
    "    subSample = np.empty([numberOfParents, 1])\n",
    "    colSampleByTree =  np.empty([numberOfParents, 1])\n",
    "\n",
    "    for i in range(numberOfParents):\n",
    "        learningRate[i] = round(random.uniform(0.01, 1), 2)\n",
    "        nEstimators[i] = random.randrange(10, 1500, step = 25)\n",
    "        maxDepth[i] = int(random.randrange(1, 10, step= 1))\n",
    "        minChildWeight[i] = round(random.uniform(0.01, 10.0), 2)\n",
    "        gammaValue[i] = round(random.uniform(0.01, 10.0), 2)\n",
    "        subSample[i] = round(random.uniform(0.01, 1.0), 2)\n",
    "        colSampleByTree[i] = round(random.uniform(0.01, 1.0), 2)\n",
    "    \n",
    "    population = np.concatenate((learningRate, nEstimators, maxDepth, minChildWeight, gammaValue, subSample, colSampleByTree), axis= 1)\n",
    "    return population\n",
    "\n",
    "   \n",
    "\n",
    "def fitness_accuracy_score(y_true, y_pred):\n",
    "    fitness = round((accuracy_score(y_true, y_pred)), 4)\n",
    "    return fitness\n",
    "\n",
    "def train_population(population, dMatrixTrain, dMatrixtest, y_test):\n",
    "    aScore = []\n",
    "    for i in range(population.shape[0]):\n",
    "        param = { 'objective':'binary:logistic',\n",
    "              'learning_rate': population[i][0],\n",
    "              'n_estimators': population[i][1], \n",
    "              'max_depth': int(population[i][2]), \n",
    "              'min_child_weight': population[i][3],\n",
    "              'gamma': population[i][4], \n",
    "              'subsample': population[i][5],\n",
    "              'colsample_bytree': population[i][6],\n",
    "              'seed': 24}\n",
    "        num_round = 100\n",
    "        xgbT = xgb.train(param, dMatrixTrain, num_round)\n",
    "        preds = xgbT.predict(dMatrixtest)\n",
    "        preds = preds>0.5\n",
    "        aScore.append(fitness_accuracy_score(y_test, preds))\n",
    "    return aScore\n",
    "\n",
    "\n",
    "\n",
    "def new_parents_selection(population, fitness, numParents):\n",
    "    selectedParents = np.empty((numParents, population.shape[1])) \n",
    "    \n",
    "    for parentId in range(numParents):\n",
    "        bestFitnessId = np.where(fitness == np.max(fitness))\n",
    "        bestFitnessId  = bestFitnessId[0][0]\n",
    "        selectedParents[parentId, :] = population[bestFitnessId, :]\n",
    "        fitness[bestFitnessId] = -1 \n",
    "    return selectedParents\n",
    "        \n",
    "\n",
    "def crossover_uniform(parents, childrenSize):\n",
    "    \n",
    "    crossoverPointIndex = np.arange(0, np.uint8(childrenSize[1]), 1, dtype= np.uint8)\n",
    "    crossoverPointIndex1 = np.random.randint(0, np.uint8(childrenSize[1]), np.uint8(childrenSize[1]/2)) \n",
    "    crossoverPointIndex2 = np.array(list(set(crossoverPointIndex) - set(crossoverPointIndex1))) \n",
    "    \n",
    "    children = np.empty(childrenSize)\n",
    "    \n",
    "    \n",
    "    for i in range(childrenSize[0]):\n",
    "        \n",
    "        parent1_index = i%parents.shape[0]\n",
    "        parent2_index = (i+1)%parents.shape[0]\n",
    "        children[i, crossoverPointIndex1] = parents[parent1_index, crossoverPointIndex1]\n",
    "        children[i, crossoverPointIndex2] = parents[parent2_index, crossoverPointIndex2]\n",
    "    return children\n",
    "    \n",
    "\n",
    "\n",
    "def mutation(crossover, numberOfParameters):\n",
    "\n",
    "    minMaxValue = np.zeros((numberOfParameters, 2))\n",
    "    \n",
    "    minMaxValue[0:] = [0.01, 1.0] \n",
    "    minMaxValue[1, :] = [10, 2000] \n",
    "    minMaxValue[2, :] = [1, 15] \n",
    "    minMaxValue[3, :] = [0, 10.0] \n",
    "    minMaxValue[4, :] = [0.01, 10.0] \n",
    "    minMaxValue[5, :] = [0.01, 1.0] \n",
    "    minMaxValue[6, :] = [0.01, 1.0] \n",
    " \n",
    "    \n",
    "    mutationValue = 0\n",
    "    parameterSelect = np.random.randint(0, 7, 1)\n",
    "    print(parameterSelect)\n",
    "    if parameterSelect == 0: \n",
    "        mutationValue = round(np.random.uniform(-0.5, 0.5), 2)\n",
    "    if parameterSelect == 1: \n",
    "        mutationValue = np.random.randint(-200, 200, 1)\n",
    "    if parameterSelect == 2:\n",
    "        mutationValue = np.random.randint(-5, 5, 1)\n",
    "    if parameterSelect == 3: \n",
    "        mutationValue = round(np.random.uniform(5, 5), 2)\n",
    "    if parameterSelect == 4: \n",
    "        mutationValue = round(np.random.uniform(-2, 2), 2)\n",
    "    if parameterSelect == 5: \n",
    "        mutationValue = round(np.random.uniform(-0.5, 0.5), 2)\n",
    "    if parameterSelect == 6: \n",
    "        mutationValue = round(np.random.uniform(-0.5, 0.5), 2)\n",
    "  \n",
    "    \n",
    "    for idx in range(crossover.shape[0]):\n",
    "        crossover[idx, parameterSelect] = crossover[idx, parameterSelect] + mutationValue\n",
    "        if(crossover[idx, parameterSelect] > minMaxValue[parameterSelect, 1]):\n",
    "            crossover[idx, parameterSelect] = minMaxValue[parameterSelect, 1]\n",
    "        if(crossover[idx, parameterSelect] < minMaxValue[parameterSelect, 0]):\n",
    "            crossover[idx, parameterSelect] = minMaxValue[parameterSelect, 0]    \n",
    "    return crossover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9f0cfc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp_param_ev_algo(X_train, X_test, y_train, y_test):\n",
    "    xgDMatrix = xgb.DMatrix(X_train, y_train) \n",
    "    xgbDMatrixTest = xgb.DMatrix(X_test, y_test)\n",
    "\n",
    "    numberOfParents = 100 \n",
    "    numberOfParentsMating = int(numberOfParents/2)\n",
    "    numberOfParameters = 7 \n",
    "    numberOfGenerations = 5\n",
    "\n",
    "    populationSize = (numberOfParents, numberOfParameters)\n",
    "    population = initilialize_poplulation(numberOfParents)\n",
    "\n",
    "    fitnessHistory = np.empty([numberOfGenerations+1, numberOfParents])\n",
    "    populationHistory = np.empty([(numberOfGenerations+1)*numberOfParents, numberOfParameters])\n",
    "    populationHistory[0:numberOfParents, :] = population\n",
    "\n",
    "    for generation in range(numberOfGenerations):\n",
    "        print(\"This is number %s generation\" % (generation))\n",
    "\n",
    "        fitnessValue = train_population(population=population, dMatrixTrain=xgDMatrix, dMatrixtest=xgbDMatrixTest, y_test=y_test)\n",
    "        fitnessHistory[generation, :] = fitnessValue\n",
    "\n",
    "        print('Best Accuracy score in the this iteration = {}'.format(np.max(fitnessHistory[generation, :])))\n",
    "\n",
    "        parents = new_parents_selection(population=population, fitness=fitnessValue, numParents=numberOfParentsMating)\n",
    "        children = crossover_uniform(parents=parents, childrenSize=(populationSize[0] - parents.shape[0], numberOfParameters))\n",
    "        children_mutated = mutation(children, numberOfParameters)\n",
    "\n",
    "        population[0:parents.shape[0], :] = parents \n",
    "        population[parents.shape[0]:, :] = children_mutated \n",
    "\n",
    "        populationHistory[(generation+1)*numberOfParents : (generation+1)*numberOfParents+ numberOfParents , :] = population \n",
    "\n",
    "    fitness = train_population(population=population, dMatrixTrain=xgDMatrix, dMatrixtest=xgbDMatrixTest, y_test=y_test)\n",
    "    fitnessHistory[generation+1, :] = fitness\n",
    "\n",
    "    bestFitnessIndex = np.where(fitness == np.max(fitness))[0][0]\n",
    "\n",
    "    print(\"Best fitness is =\", fitness[bestFitnessIndex])\n",
    "\n",
    "\n",
    "    print(\"Best parameters are:\")\n",
    "    print('learning_rate', population[bestFitnessIndex][0])\n",
    "    print('n_estimators', population[bestFitnessIndex][1])\n",
    "    print('max_depth', int(population[bestFitnessIndex][2])) \n",
    "    print('min_child_weight', population[bestFitnessIndex][3])\n",
    "    print('gamma', population[bestFitnessIndex][4])\n",
    "    print('subsample', population[bestFitnessIndex][5])\n",
    "    print('colsample_bytree', population[bestFitnessIndex][6])\n",
    "    \n",
    "    return population[bestFitnessIndex][0],population[bestFitnessIndex][1],population[bestFitnessIndex][2],population[bestFitnessIndex][3],population[bestFitnessIndex][4],population[bestFitnessIndex][5],population[bestFitnessIndex][6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aa068d",
   "metadata": {},
   "source": [
    "### Forward (first RFE, then tuning hyperparameters on selected features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7eacc83a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-1f2ecc535109>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#test tuning XGB hyperparameters with selected features from RFE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnewX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnewX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrfe_xgb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_child_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubsample\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolsample_bytree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhyp_param_ev_algo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mXGB_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnewX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_child_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubsample\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolsample_bytree\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msimple\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-52-e996644f4633>\u001b[0m in \u001b[0;36mrfe_xgb\u001b[1;34m(X_train, y_train)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muse_label_encoder\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mrfecv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRFECV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin_features_to_select\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_features_to_select\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"r2\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mrfecv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m#keep selected features + check RFE accuracy scores during running\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    708\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdelayed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_rfe_single_fit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 710\u001b[1;33m         scores = parallel(\n\u001b[0m\u001b[0;32m    711\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrfe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1054\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1055\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    931\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    432\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#test tuning XGB hyperparameters with selected features from RFE\n",
    "newX_train,newX_test = rfe_xgb(X_train, y_train)\n",
    "learning_rate, n_estimators, max_depth, min_child_weight,gamma, subsample,colsample_bytree = hyp_param_ev_algo(newX_train, newX_test, y_train, y_test)\n",
    "XGB_class(newX_train,newX_test,y_train,y_test,learning_rate, n_estimators, max_depth, min_child_weight,gamma, subsample,colsample_bytree,simple=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a8aa71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
