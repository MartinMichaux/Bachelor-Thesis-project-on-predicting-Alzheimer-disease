{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70e18535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import poisson\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import xgboost as xgb\n",
    "xgb.set_config(verbosity=0)\n",
    "from sklearn.feature_selection import RFECV\n",
    "import random\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4e3475",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c8f6110",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset\\CSF_Proteomics_ADNI.csv\")\n",
    "\n",
    "#replace categorical feature with index labeling\n",
    "df['binary_class'].replace({'stable':0,'decliner':1},inplace=True)\n",
    "df['three_class'].replace({'slowDecline':0,'rapidDecline':1,'stable':2},inplace=True)\n",
    "\n",
    "#differentiate other categorical features from the numerical ones\n",
    "pheno = df.loc[:,'RID':'VISCODE']\n",
    "data = df.loc[:,'A1AT.AVLTIDEK':'VTDB.VPTADLEDVLPLAEDITNILSK']\n",
    "\n",
    "## Normalization\n",
    "\n",
    "#normally test function -> check if each column is normally distr\n",
    "def norm_test(data):\n",
    "    alpha = 1e-3\n",
    "    k2, p = stats.normaltest(data)\n",
    "    count=0\n",
    "    for i in p:\n",
    "        if i > alpha:  # null hypothesis: x comes from a normal distribution\n",
    "            count+=1\n",
    "    print('There are ',count,'normally distributed features out of',data.shape[1])\n",
    "\n",
    "#QUANTILE NORMALIZATION\n",
    "def quantile_normalize(df):\n",
    "    df_sorted = pd.DataFrame(np.sort(df.values,axis=0),index=df.index,columns=df.columns)\n",
    "    df_mean = df_sorted.mean(axis=1)\n",
    "    df_mean.index = np.arange(1, len(df_mean) + 1)\n",
    "    df_qn =df.rank(method=\"min\").stack().astype(int).map(df_mean).unstack()\n",
    "    return df_qn\n",
    "\n",
    "# compute quantile normalized data\n",
    "df_qn=quantile_normalize(data)\n",
    "data = df_qn\n",
    "\n",
    "### Box-cox transformation\n",
    "df_bc = pd.DataFrame().reindex_like(data)\n",
    "\n",
    "for col in df_qn:\n",
    "    df_bc[col],_ = stats.boxcox(df_qn[col])\n",
    "data = df_qn\n",
    "\n",
    "#Scale the data to the range between 0 and 1 before using PCA\n",
    "scaler = MinMaxScaler()\n",
    "data_rescaled = scaler.fit_transform(data)\n",
    "\n",
    "## Split dataset\n",
    "X = data_rescaled\n",
    "y = df[\"binary_class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c797dc",
   "metadata": {},
   "source": [
    "## Test models using with PCA as feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28eebb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note that we now have 13 columns instead of 320\n"
     ]
    }
   ],
   "source": [
    "# Make an instance of the model keeping 95% of the features variance\n",
    "pca = PCA(0.8)\n",
    "# pca = PCA(n_components=100)\n",
    "\n",
    "principalComponents = pca.fit_transform(X)\n",
    "principalDf = pd.DataFrame(data = principalComponents)\n",
    "print('Note that we now have',principalDf.shape[1],'columns instead of',X.shape[1])\n",
    "\n",
    "X_train_20, X_test_20, y_train_20, y_test_20 = train_test_split(principalDf, y, test_size = 0.20, random_state = 97, stratify = y)\n",
    "X_train_40, X_test_40, y_train_40, y_test_40 = train_test_split(principalDf, y, test_size = 0.40, random_state = 97, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac6cfd06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "PC_values = np.arange(pca.n_components_)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.title('Cumulative explained variance ratio')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af91e1e5",
   "metadata": {},
   "source": [
    "### Single xgb classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a3282d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGB_class(X_train, X_test, y_train, y_test,learning_rate, n_estimators, max_depth,min_child_weight, gamma, subsample, colsample_bytree, simple):\n",
    "    \n",
    "    if simple:\n",
    "        clf = xgb.XGBClassifier(seed = 24, use_label_encoder =False)\n",
    "    else:\n",
    "        clf = xgb.XGBClassifier(learning_rate = learning_rate, n_estimators = int(n_estimators), max_depth = int(max_depth), \n",
    "                                min_child_weight = min_child_weight, gamma = gamma, subsample = subsample, \n",
    "                                colsample_bytree = colsample_bytree, seed = 24,eval_metric='mlogloss',use_label_encoder =False)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_predicted = clf.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, y_predicted))\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_predicted))\n",
    "    print(\"MCC =\",matthews_corrcoef(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bb59e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 20% testing subset\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.30      0.33        10\n",
      "           1       0.63      0.71      0.67        17\n",
      "\n",
      "    accuracy                           0.56        27\n",
      "   macro avg       0.50      0.50      0.50        27\n",
      "weighted avg       0.54      0.56      0.54        27\n",
      "\n",
      "Accuracy: 0.5555555555555556\n",
      "MCC = 0.006220907522417994\n",
      "Using 40% testing subset\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.43      0.46        21\n",
      "           1       0.67      0.73      0.70        33\n",
      "\n",
      "    accuracy                           0.61        54\n",
      "   macro avg       0.58      0.58      0.58        54\n",
      "weighted avg       0.60      0.61      0.60        54\n",
      "\n",
      "Accuracy: 0.6111111111111112\n",
      "MCC = 0.16116459280507606\n"
     ]
    }
   ],
   "source": [
    "print('Using 20% testing subset:')\n",
    "XGB_class(X_train_20,X_test_20,y_train_20,y_test_20,0,0,0,0,0,0,0,simple=True)\n",
    "print('Using 40% testing subset:')\n",
    "XGB_class(X_train_40,X_test_40,y_train_40,y_test_40,0,0,0,0,0,0,0,simple=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9601563c",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06226111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_class(X_train,X_test,y_train,y_test):\n",
    "    clf = SVC(kernel='linear') \n",
    "    clf.fit(X_train, y_train) \n",
    "    y_predicted = clf.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, y_predicted))\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_predicted))\n",
    "    print(\"MCC =\",matthews_corrcoef(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88e6f378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 20% testing subset:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.50      0.53        10\n",
      "           1       0.72      0.76      0.74        17\n",
      "\n",
      "    accuracy                           0.67        27\n",
      "   macro avg       0.64      0.63      0.63        27\n",
      "weighted avg       0.66      0.67      0.66        27\n",
      "\n",
      "Accuracy: 0.6666666666666666\n",
      "MCC = 0.27116307227332026\n",
      "Using 40% testing subset:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.48      0.56        21\n",
      "           1       0.72      0.85      0.78        33\n",
      "\n",
      "    accuracy                           0.70        54\n",
      "   macro avg       0.69      0.66      0.67        54\n",
      "weighted avg       0.70      0.70      0.69        54\n",
      "\n",
      "Accuracy: 0.7037037037037037\n",
      "MCC = 0.3533767463701098\n"
     ]
    }
   ],
   "source": [
    "print('Using 20% testing subset:')\n",
    "SVM_class(X_train_20,X_test_20,y_train_20,y_test_20)\n",
    "print('Using 40% testing subset:')\n",
    "SVM_class(X_train_40,X_test_40,y_train_40,y_test_40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e5ad9a",
   "metadata": {},
   "source": [
    "### Single Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e1689f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SRF_class(X_train,X_test,y_train,y_test):\n",
    "    model = RandomForestClassifier().fit(X_train,y_train)\n",
    "    y_predicted = model.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, y_predicted))\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_predicted))\n",
    "    print(\"MCC =\",matthews_corrcoef(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4edcd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 20% testing subset:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.20      0.25        10\n",
      "           1       0.62      0.76      0.68        17\n",
      "\n",
      "    accuracy                           0.56        27\n",
      "   macro avg       0.48      0.48      0.47        27\n",
      "weighted avg       0.51      0.56      0.52        27\n",
      "\n",
      "Accuracy: 0.5555555555555556\n",
      "MCC = -0.04099600308453939\n",
      "Using 40% testing subset:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.29      0.40        21\n",
      "           1       0.67      0.91      0.77        33\n",
      "\n",
      "    accuracy                           0.67        54\n",
      "   macro avg       0.67      0.60      0.58        54\n",
      "weighted avg       0.67      0.67      0.63        54\n",
      "\n",
      "Accuracy: 0.6666666666666666\n",
      "MCC = 0.25482359571881275\n"
     ]
    }
   ],
   "source": [
    "print('Using 20% testing subset:')\n",
    "SRF_class(X_train_20,X_test_20,y_train_20,y_test_20)\n",
    "print('Using 40% testing subset:')\n",
    "SRF_class(X_train_40,X_test_40,y_train_40,y_test_40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc17aa7b",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce711b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR_class(X_train,X_test,y_train,y_test):\n",
    "    model = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "    y_predicted = model.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, y_predicted))\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_predicted))\n",
    "    print(\"MCC =\",matthews_corrcoef(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ec8a707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 20% testing subset:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.40      0.44        10\n",
      "           1       0.68      0.76      0.72        17\n",
      "\n",
      "    accuracy                           0.63        27\n",
      "   macro avg       0.59      0.58      0.58        27\n",
      "weighted avg       0.62      0.63      0.62        27\n",
      "\n",
      "Accuracy: 0.6296296296296297\n",
      "MCC = 0.17418541062770382\n",
      "Using 40% testing subset:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.52      0.59        21\n",
      "           1       0.74      0.85      0.79        33\n",
      "\n",
      "    accuracy                           0.72        54\n",
      "   macro avg       0.71      0.69      0.69        54\n",
      "weighted avg       0.72      0.72      0.71        54\n",
      "\n",
      "Accuracy: 0.7222222222222222\n",
      "MCC = 0.3974672033225129\n"
     ]
    }
   ],
   "source": [
    "print('Using 20% testing subset:')\n",
    "LR_class(X_train_20,X_test_20,y_train_20,y_test_20)\n",
    "print('Using 40% testing subset:')\n",
    "LR_class(X_train_40,X_test_40,y_train_40,y_test_40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0ee59b",
   "metadata": {},
   "source": [
    "### Test which PCA variance is the best for XGBoost classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dce5afdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "variances = np.arange(0.5, 0.99, 0.01).tolist()\n",
    "nbr_tests = 2\n",
    "\n",
    "# Creates a list containing 5 lists, each of 8 items, all set to 0\n",
    "w, h = len(variances), nbr_tests\n",
    "perf = [[0 for x in range(w)] for y in range(h)] \n",
    "\n",
    "for i in range(nbr_tests):\n",
    "    for j in range(len(variances)):\n",
    "        pca = PCA(variances[j])\n",
    "\n",
    "        principalComponents = pca.fit_transform(X)\n",
    "        principalDf = pd.DataFrame(data = principalComponents)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(principalDf, y, test_size = 0.20, random_state = 97)\n",
    "\n",
    "        clf = xgb.XGBClassifier(random_state = random.randint(0,nbr_tests),use_label_encoder =False)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_predicted = clf.predict(X_test)\n",
    "\n",
    "        acc = accuracy_score(y_test, y_predicted) \n",
    "        perf[i][j] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "da58e01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4074074074074074, 0.4074074074074074, 0.4074074074074074, 0.4074074074074074, 0.4074074074074074, 0.4074074074074074, 0.4074074074074074, 0.4074074074074074, 0.4074074074074074, 0.4074074074074074, 0.4074074074074074, 0.4074074074074074, 0.4074074074074074, 0.4074074074074074, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.4444444444444444, 0.4074074074074074, 0.4074074074074074, 0.37037037037037035, 0.5185185185185185, 0.5555555555555556, 0.5555555555555556, 0.5925925925925926, 0.6296296296296297, 0.6296296296296297, 0.6296296296296297, 0.5555555555555556, 0.5925925925925926, 0.5925925925925926, 0.5555555555555556, 0.5185185185185185, 0.48148148148148145, 0.4444444444444444, 0.4444444444444444, 0.48148148148148145, 0.37037037037037035, 0.5185185185185185, 0.4444444444444444, 0.5185185185185185, 0.48148148148148145, 0.48148148148148145, 0.48148148148148145, 0.5555555555555556]\n"
     ]
    }
   ],
   "source": [
    "print(perf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed8a23e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (49,) and (2, 49)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-aac23f0705e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariances\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Variance'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'XGBoost accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"XGBoost performance on the PCA variances\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2838\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2839\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2840\u001b[1;33m     return gca().plot(\n\u001b[0m\u001b[0;32m   2841\u001b[0m         \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2842\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1741\u001b[0m         \"\"\"\n\u001b[0;32m   1742\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1743\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1744\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    271\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 273\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 399\u001b[1;33m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[0;32m    400\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[0;32m    401\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (49,) and (2, 49)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(variances, perf)\n",
    "plt.xlabel('Variance') \n",
    "plt.ylabel('XGBoost accuracy') \n",
    "plt.title(\"XGBoost performance on the PCA variances\")\n",
    "plt.show()\n",
    "print(\"So best variance with the XGBoost Classifier model is 0.8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84fd2d8",
   "metadata": {},
   "source": [
    "## test with strong model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "86eaec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameter tuning XGBoost classifier\n",
    "\n",
    "random.seed(723)\n",
    "np.random.seed(723)\n",
    "\n",
    "def initilialize_poplulation(numberOfParents):\n",
    "    learningRate = np.empty([numberOfParents, 1])\n",
    "    nEstimators = np.empty([numberOfParents, 1], dtype = np.uint8)\n",
    "    maxDepth = np.empty([numberOfParents, 1], dtype = np.uint8)\n",
    "    minChildWeight = np.empty([numberOfParents, 1])\n",
    "    gammaValue = np.empty([numberOfParents, 1])\n",
    "    subSample = np.empty([numberOfParents, 1])\n",
    "    colSampleByTree =  np.empty([numberOfParents, 1])\n",
    "\n",
    "    for i in range(numberOfParents):\n",
    "        learningRate[i] = round(random.uniform(0.01, 1), 2)\n",
    "        nEstimators[i] = random.randrange(10, 1500, step = 25)\n",
    "        maxDepth[i] = int(random.randrange(1, 10, step= 1))\n",
    "        minChildWeight[i] = round(random.uniform(0.01, 10.0), 2)\n",
    "        gammaValue[i] = round(random.uniform(0.01, 10.0), 2)\n",
    "        subSample[i] = round(random.uniform(0.01, 1.0), 2)\n",
    "        colSampleByTree[i] = round(random.uniform(0.01, 1.0), 2)\n",
    "    \n",
    "    population = np.concatenate((learningRate, nEstimators, maxDepth, minChildWeight, gammaValue, subSample, colSampleByTree), axis= 1)\n",
    "    return population\n",
    "\n",
    "   \n",
    "\n",
    "def fitness_accuracy_score(y_true, y_pred):\n",
    "    fitness = round((accuracy_score(y_true, y_pred)), 4)\n",
    "    return fitness\n",
    "\n",
    "def train_population(population, dMatrixTrain, dMatrixtest, y_test):\n",
    "    aScore = []\n",
    "    for i in range(population.shape[0]):\n",
    "        param = { 'objective':'binary:logistic',\n",
    "              'learning_rate': population[i][0],\n",
    "              'n_estimators': population[i][1], \n",
    "              'max_depth': int(population[i][2]), \n",
    "              'min_child_weight': population[i][3],\n",
    "              'gamma': population[i][4], \n",
    "              'subsample': population[i][5],\n",
    "              'colsample_bytree': population[i][6],\n",
    "              'seed': 24}\n",
    "        num_round = 100\n",
    "        xgbT = xgb.train(param, dMatrixTrain, num_round)\n",
    "        preds = xgbT.predict(dMatrixtest)\n",
    "        preds = preds>0.5\n",
    "        aScore.append(fitness_accuracy_score(y_test, preds))\n",
    "    return aScore\n",
    "\n",
    "\n",
    "\n",
    "def new_parents_selection(population, fitness, numParents):\n",
    "    selectedParents = np.empty((numParents, population.shape[1])) \n",
    "    \n",
    "    for parentId in range(numParents):\n",
    "        bestFitnessId = np.where(fitness == np.max(fitness))\n",
    "        bestFitnessId  = bestFitnessId[0][0]\n",
    "        selectedParents[parentId, :] = population[bestFitnessId, :]\n",
    "        fitness[bestFitnessId] = -1 \n",
    "    return selectedParents\n",
    "        \n",
    "\n",
    "def crossover_uniform(parents, childrenSize):\n",
    "    \n",
    "    crossoverPointIndex = np.arange(0, np.uint8(childrenSize[1]), 1, dtype= np.uint8)\n",
    "    crossoverPointIndex1 = np.random.randint(0, np.uint8(childrenSize[1]), np.uint8(childrenSize[1]/2)) \n",
    "    crossoverPointIndex2 = np.array(list(set(crossoverPointIndex) - set(crossoverPointIndex1))) \n",
    "    \n",
    "    children = np.empty(childrenSize)\n",
    "    \n",
    "    \n",
    "    for i in range(childrenSize[0]):\n",
    "        \n",
    "        parent1_index = i%parents.shape[0]\n",
    "        parent2_index = (i+1)%parents.shape[0]\n",
    "        children[i, crossoverPointIndex1] = parents[parent1_index, crossoverPointIndex1]\n",
    "        children[i, crossoverPointIndex2] = parents[parent2_index, crossoverPointIndex2]\n",
    "    return children\n",
    "    \n",
    "\n",
    "\n",
    "def mutation(crossover, numberOfParameters):\n",
    "\n",
    "    minMaxValue = np.zeros((numberOfParameters, 2))\n",
    "    \n",
    "    minMaxValue[0:] = [0.01, 1.0] \n",
    "    minMaxValue[1, :] = [10, 2000] \n",
    "    minMaxValue[2, :] = [1, 15] \n",
    "    minMaxValue[3, :] = [0, 10.0] \n",
    "    minMaxValue[4, :] = [0.01, 10.0] \n",
    "    minMaxValue[5, :] = [0.01, 1.0] \n",
    "    minMaxValue[6, :] = [0.01, 1.0] \n",
    " \n",
    "    \n",
    "    mutationValue = 0\n",
    "    parameterSelect = np.random.randint(0, 7, 1)\n",
    "    print(parameterSelect)\n",
    "    if parameterSelect == 0: \n",
    "        mutationValue = round(np.random.uniform(-0.5, 0.5), 2)\n",
    "    if parameterSelect == 1: \n",
    "        mutationValue = np.random.randint(-200, 200, 1)\n",
    "    if parameterSelect == 2:\n",
    "        mutationValue = np.random.randint(-5, 5, 1)\n",
    "    if parameterSelect == 3: \n",
    "        mutationValue = round(np.random.uniform(5, 5), 2)\n",
    "    if parameterSelect == 4: \n",
    "        mutationValue = round(np.random.uniform(-2, 2), 2)\n",
    "    if parameterSelect == 5: \n",
    "        mutationValue = round(np.random.uniform(-0.5, 0.5), 2)\n",
    "    if parameterSelect == 6: \n",
    "        mutationValue = round(np.random.uniform(-0.5, 0.5), 2)\n",
    "  \n",
    "    \n",
    "    for idx in range(crossover.shape[0]):\n",
    "        crossover[idx, parameterSelect] = crossover[idx, parameterSelect] + mutationValue\n",
    "        if(crossover[idx, parameterSelect] > minMaxValue[parameterSelect, 1]):\n",
    "            crossover[idx, parameterSelect] = minMaxValue[parameterSelect, 1]\n",
    "        if(crossover[idx, parameterSelect] < minMaxValue[parameterSelect, 0]):\n",
    "            crossover[idx, parameterSelect] = minMaxValue[parameterSelect, 0]    \n",
    "    return crossover\n",
    "\n",
    "def hyp_param_ev_algo(X_train, X_test, y_train, y_test):\n",
    "    xgDMatrix = xgb.DMatrix(X_train, y_train) \n",
    "    xgbDMatrixTest = xgb.DMatrix(X_test, y_test)\n",
    "\n",
    "    numberOfParents = 100 \n",
    "    numberOfParentsMating = int(numberOfParents/2)\n",
    "    numberOfParameters = 7 \n",
    "    numberOfGenerations = 100\n",
    "\n",
    "    populationSize = (numberOfParents, numberOfParameters)\n",
    "    population = initilialize_poplulation(numberOfParents)\n",
    "\n",
    "    fitnessHistory = np.empty([numberOfGenerations+1, numberOfParents])\n",
    "    populationHistory = np.empty([(numberOfGenerations+1)*numberOfParents, numberOfParameters])\n",
    "    populationHistory[0:numberOfParents, :] = population\n",
    "\n",
    "    for generation in range(numberOfGenerations):\n",
    "        print(\"This is number %s generation\" % (generation))\n",
    "\n",
    "        fitnessValue = train_population(population=population, dMatrixTrain=xgDMatrix, dMatrixtest=xgbDMatrixTest, y_test=y_test)\n",
    "        fitnessHistory[generation, :] = fitnessValue\n",
    "\n",
    "        print('Best Accuracy score in the this iteration = {}'.format(np.max(fitnessHistory[generation, :])))\n",
    "\n",
    "        parents = new_parents_selection(population=population, fitness=fitnessValue, numParents=numberOfParentsMating)\n",
    "        children = crossover_uniform(parents=parents, childrenSize=(populationSize[0] - parents.shape[0], numberOfParameters))\n",
    "        children_mutated = mutation(children, numberOfParameters)\n",
    "\n",
    "        population[0:parents.shape[0], :] = parents \n",
    "        population[parents.shape[0]:, :] = children_mutated \n",
    "\n",
    "        populationHistory[(generation+1)*numberOfParents : (generation+1)*numberOfParents+ numberOfParents , :] = population \n",
    "\n",
    "    fitness = train_population(population=population, dMatrixTrain=xgDMatrix, dMatrixtest=xgbDMatrixTest, y_test=y_test)\n",
    "    fitnessHistory[generation+1, :] = fitness\n",
    "\n",
    "    bestFitnessIndex = np.where(fitness == np.max(fitness))[0][0]\n",
    "\n",
    "    print(\"Best fitness is =\", fitness[bestFitnessIndex])\n",
    "\n",
    "\n",
    "    print(\"Best parameters are:\")\n",
    "    print('learning_rate', population[bestFitnessIndex][0])\n",
    "    print('n_estimators', population[bestFitnessIndex][1])\n",
    "    print('max_depth', int(population[bestFitnessIndex][2])) \n",
    "    print('min_child_weight', population[bestFitnessIndex][3])\n",
    "    print('gamma', population[bestFitnessIndex][4])\n",
    "    print('subsample', population[bestFitnessIndex][5])\n",
    "    print('colsample_bytree', population[bestFitnessIndex][6])\n",
    "    \n",
    "    return population[bestFitnessIndex][0],population[bestFitnessIndex][1],population[bestFitnessIndex][2],population[bestFitnessIndex][3],population[bestFitnessIndex][4],population[bestFitnessIndex][5],population[bestFitnessIndex][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f6b08641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is number 0 generation\n",
      "Best Accuracy score in the this iteration = 0.7407\n",
      "[2]\n",
      "This is number 1 generation\n",
      "Best Accuracy score in the this iteration = 0.7407\n",
      "[2]\n",
      "This is number 2 generation\n",
      "Best Accuracy score in the this iteration = 0.7407\n",
      "[1]\n",
      "This is number 3 generation\n",
      "Best Accuracy score in the this iteration = 0.7407\n",
      "[2]\n",
      "This is number 4 generation\n",
      "Best Accuracy score in the this iteration = 0.7407\n",
      "[4]\n",
      "This is number 5 generation\n",
      "Best Accuracy score in the this iteration = 0.7407\n",
      "[1]\n",
      "This is number 6 generation\n",
      "Best Accuracy score in the this iteration = 0.7407\n",
      "[2]\n",
      "This is number 7 generation\n",
      "Best Accuracy score in the this iteration = 0.7407\n",
      "[4]\n",
      "This is number 8 generation\n",
      "Best Accuracy score in the this iteration = 0.7407\n",
      "[1]\n",
      "This is number 9 generation\n",
      "Best Accuracy score in the this iteration = 0.7778\n",
      "[3]\n",
      "This is number 10 generation\n",
      "Best Accuracy score in the this iteration = 0.7778\n",
      "[0]\n",
      "This is number 11 generation\n",
      "Best Accuracy score in the this iteration = 0.7778\n",
      "[0]\n",
      "This is number 12 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[3]\n",
      "This is number 13 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[6]\n",
      "This is number 14 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[4]\n",
      "This is number 15 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[3]\n",
      "This is number 16 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[3]\n",
      "This is number 17 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[4]\n",
      "This is number 18 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[4]\n",
      "This is number 19 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[3]\n",
      "This is number 20 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[2]\n",
      "This is number 21 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[3]\n",
      "This is number 22 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[0]\n",
      "This is number 23 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[0]\n",
      "This is number 24 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[5]\n",
      "This is number 25 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[5]\n",
      "This is number 26 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[5]\n",
      "This is number 27 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[2]\n",
      "This is number 28 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[3]\n",
      "This is number 29 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[0]\n",
      "This is number 30 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[1]\n",
      "This is number 31 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[3]\n",
      "This is number 32 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[1]\n",
      "This is number 33 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[3]\n",
      "This is number 34 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[0]\n",
      "This is number 35 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[6]\n",
      "This is number 36 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[2]\n",
      "This is number 37 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[1]\n",
      "This is number 38 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[5]\n",
      "This is number 39 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[4]\n",
      "This is number 40 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[2]\n",
      "This is number 41 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[5]\n",
      "This is number 42 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[6]\n",
      "This is number 43 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[4]\n",
      "This is number 44 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[4]\n",
      "This is number 45 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[2]\n",
      "This is number 46 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[2]\n",
      "This is number 47 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[6]\n",
      "This is number 48 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[4]\n",
      "This is number 49 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[0]\n",
      "This is number 50 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[0]\n",
      "This is number 51 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[3]\n",
      "This is number 52 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[2]\n",
      "This is number 53 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[3]\n",
      "This is number 54 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[5]\n",
      "This is number 55 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[0]\n",
      "This is number 56 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[0]\n",
      "This is number 57 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[6]\n",
      "This is number 58 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[1]\n",
      "This is number 59 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[0]\n",
      "This is number 60 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[4]\n",
      "This is number 61 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[6]\n",
      "This is number 62 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[1]\n",
      "This is number 63 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[5]\n",
      "This is number 64 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[6]\n",
      "This is number 65 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[1]\n",
      "This is number 66 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[3]\n",
      "This is number 67 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[2]\n",
      "This is number 68 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[5]\n",
      "This is number 69 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[4]\n",
      "This is number 70 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[2]\n",
      "This is number 71 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[0]\n",
      "This is number 72 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[6]\n",
      "This is number 73 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[5]\n",
      "This is number 74 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[3]\n",
      "This is number 75 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[0]\n",
      "This is number 76 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[6]\n",
      "This is number 77 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[6]\n",
      "This is number 78 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[0]\n",
      "This is number 79 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[0]\n",
      "This is number 80 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[6]\n",
      "This is number 81 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[1]\n",
      "This is number 82 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[1]\n",
      "This is number 83 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[0]\n",
      "This is number 84 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[5]\n",
      "This is number 85 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[0]\n",
      "This is number 86 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[6]\n",
      "This is number 87 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[1]\n",
      "This is number 88 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[4]\n",
      "This is number 89 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[4]\n",
      "This is number 90 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[4]\n",
      "This is number 91 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[6]\n",
      "This is number 92 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[2]\n",
      "This is number 93 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[3]\n",
      "This is number 94 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[4]\n",
      "This is number 95 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[1]\n",
      "This is number 96 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[6]\n",
      "This is number 97 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[6]\n",
      "This is number 98 generation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[1]\n",
      "This is number 99 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[5]\n",
      "Best fitness is = 0.8889\n",
      "Best parameters are:\n",
      "learning_rate 0.61\n",
      "n_estimators 129.0\n",
      "max_depth 3\n",
      "min_child_weight 6.74\n",
      "gamma 3.8500000000000005\n",
      "subsample 0.58\n",
      "colsample_bytree 0.21\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.73      0.76        11\n",
      "           1       0.82      0.88      0.85        16\n",
      "\n",
      "    accuracy                           0.81        27\n",
      "   macro avg       0.81      0.80      0.81        27\n",
      "weighted avg       0.81      0.81      0.81        27\n",
      "\n",
      "Accuracy: 0.8148148148148148\n",
      "MCC = 0.6128089093333163\n"
     ]
    }
   ],
   "source": [
    "#test RFE with tuning XGB hyperparameters\n",
    "learning_rate, n_estimators, max_depth, min_child_weight,gamma, subsample,colsample_bytree = hyp_param_ev_algo(X_train, X_test, y_train, y_test)\n",
    "XGB_class(X_train,X_test,y_train,y_test,learning_rate, n_estimators, max_depth, min_child_weight,gamma, subsample,colsample_bytree,simple=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75902491",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
