{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83a83341",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d435b1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import poisson\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import xgboost as xgb\n",
    "xgb.set_config(verbosity=0)\n",
    "from sklearn.feature_selection import RFECV\n",
    "import random\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29a92679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop first column of dataframe\n",
    "def drop_first_col(df):\n",
    "    return df.iloc[: , 1:]\n",
    "\n",
    "X_train_20 = pd.read_csv(\"dataset\\X_train_3_20.csv\")\n",
    "X_test_20 = pd.read_csv(\"dataset\\X_test_3_20.csv\")\n",
    "y_train_20 = pd.read_csv(\"dataset\\y_train_3_20.csv\")\n",
    "y_test_20 = pd.read_csv(\"dataset\\y_test_3_20.csv\")\n",
    "\n",
    "X_train_40 = pd.read_csv(\"dataset\\X_train_3_40.csv\")\n",
    "X_test_40 = pd.read_csv(\"dataset\\X_test_3_40.csv\")\n",
    "y_train_40 = pd.read_csv(\"dataset\\y_train_3_40.csv\")\n",
    "y_test_40 = pd.read_csv(\"dataset\\y_test_3_40.csv\")\n",
    "\n",
    "X_train_20 = drop_first_col(X_train_20)\n",
    "X_test_20 = drop_first_col(X_test_20)\n",
    "y_train_20 = drop_first_col(y_train_20)\n",
    "y_test_20 = drop_first_col(y_test_20)\n",
    "\n",
    "X_train_40 = drop_first_col(X_train_40)\n",
    "X_test_40 = drop_first_col(X_test_40)\n",
    "y_train_40 = drop_first_col(y_train_40)\n",
    "y_test_40 = drop_first_col(y_test_40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c6482",
   "metadata": {},
   "source": [
    "## Test on single simple models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c80bba",
   "metadata": {},
   "source": [
    "### Single XGB Classifier (with no hyperparameters selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc922337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGB_class(X_train, X_test, y_train, y_test,learning_rate, n_estimators, max_depth,min_child_weight, gamma, subsample, colsample_bytree, simple):\n",
    "    \n",
    "    if simple:\n",
    "        clf = xgb.XGBClassifier(seed = 24, use_label_encoder =False)\n",
    "    else:\n",
    "        clf = xgb.XGBClassifier(learning_rate = learning_rate, n_estimators = int(n_estimators), max_depth = int(max_depth), \n",
    "                                min_child_weight = min_child_weight, gamma = gamma, subsample = subsample, \n",
    "                                colsample_bytree = colsample_bytree, seed = 24,eval_metric='mlogloss',use_label_encoder =False)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_predicted = clf.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, y_predicted))\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_predicted))\n",
    "    print(\"MCC =\",matthews_corrcoef(y_test, y_predicted))\n",
    "    print(y_test)\n",
    "    for i in range(3):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_predicted[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    print(\"AUC =\",roc_auc)\n",
    "    \n",
    "    return clf.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c01f28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.56      0.50         9\n",
      "           1       0.70      0.70      0.70        10\n",
      "           2       0.50      0.38      0.43         8\n",
      "\n",
      "    accuracy                           0.56        27\n",
      "   macro avg       0.55      0.54      0.54        27\n",
      "weighted avg       0.56      0.56      0.55        27\n",
      "\n",
      "Accuracy: 0.5555555555555556\n",
      "MCC = 0.3305699764271252\n",
      "    three_class\n",
      "0             1\n",
      "1             1\n",
      "2             2\n",
      "3             1\n",
      "4             1\n",
      "5             0\n",
      "6             0\n",
      "7             0\n",
      "8             0\n",
      "9             2\n",
      "10            0\n",
      "11            1\n",
      "12            1\n",
      "13            2\n",
      "14            2\n",
      "15            1\n",
      "16            2\n",
      "17            1\n",
      "18            2\n",
      "19            0\n",
      "20            1\n",
      "21            0\n",
      "22            0\n",
      "23            1\n",
      "24            2\n",
      "25            2\n",
      "26            0\n"
     ]
    },
    {
     "ename": "InvalidIndexError",
     "evalue": "(slice(None, None, None), 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:142\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '(slice(None, None, None), 0)' is an invalid key",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sXGBc_y_predicted_20 \u001b[38;5;241m=\u001b[39m \u001b[43mXGB_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_20\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_test_20\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train_20\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_test_20\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43msimple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m sXGBc_y_predicted_40 \u001b[38;5;241m=\u001b[39m XGB_class(X_train_40,X_test_40,y_train_40,y_test_40,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,simple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mXGB_class\u001b[1;34m(X_train, X_test, y_train, y_test, learning_rate, n_estimators, max_depth, min_child_weight, gamma, subsample, colsample_bytree, simple)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_test)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m---> 18\u001b[0m     fpr[i], tpr[i], _ \u001b[38;5;241m=\u001b[39m roc_curve(\u001b[43my_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m, y_predicted[:, i])\n\u001b[0;32m     19\u001b[0m     roc_auc[i] \u001b[38;5;241m=\u001b[39m auc(fpr[i], tpr[i])\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAUC =\u001b[39m\u001b[38;5;124m\"\u001b[39m,roc_auc)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3628\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3623\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m         \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m         \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m         \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m-> 3628\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_indexing_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3629\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m   3631\u001b[0m \u001b[38;5;66;03m# GH#42269\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5637\u001b[0m, in \u001b[0;36mIndex._check_indexing_error\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   5633\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_indexing_error\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m   5634\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(key):\n\u001b[0;32m   5635\u001b[0m         \u001b[38;5;66;03m# if key is not a scalar, directly raise an error (the code below\u001b[39;00m\n\u001b[0;32m   5636\u001b[0m         \u001b[38;5;66;03m# would convert to numpy arrays and raise later any way) - GH29926\u001b[39;00m\n\u001b[1;32m-> 5637\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n",
      "\u001b[1;31mInvalidIndexError\u001b[0m: (slice(None, None, None), 0)"
     ]
    }
   ],
   "source": [
    "sXGBc_y_predicted_20 = XGB_class(X_train_20,X_test_20,y_train_20,y_test_20,0,0,0,0,0,0,0,simple=True)\n",
    "sXGBc_y_predicted_40 = XGB_class(X_train_40,X_test_40,y_train_40,y_test_40,0,0,0,0,0,0,0,simple=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8885d3",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2de33db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sSVMc(X_train,X_test,y_train,y_test):\n",
    "    clf = SVC(kernel='linear',probability=True) \n",
    "    clf.fit(X_train, y_train) \n",
    "    preds = clf.predict(X_test)\n",
    "    sSVM_y_predicted = clf.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    print(classification_report(y_test, preds))\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, preds))\n",
    "    print(\"MCC =\",matthews_corrcoef(y_test, preds))\n",
    "    \n",
    "    return sSVM_y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c7d574",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Using 20% as test subset:')\n",
    "sSVM_y_predicted_20 = sSVMc(X_train_20,X_test_20,y_train_20,y_test_20)\n",
    "print('Using 40% as test subset:')\n",
    "sSVM_y_predicted_40 = sSVMc(X_train_40,X_test_40,y_train_40,y_test_40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df80d3c",
   "metadata": {},
   "source": [
    "### Single Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7c64ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sRFC(X_train,X_test,y_train,y_test):\n",
    "    model = RandomForestClassifier().fit(X_train,y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    sRFC_y_predicted = model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    print(classification_report(y_test, preds))\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, preds))\n",
    "    print(\"MCC =\",matthews_corrcoef(y_test, preds))\n",
    "    \n",
    "    return sRFC_y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09f5463",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Using 20% as test subset:')\n",
    "sRFC_y_predicted_20 = sRFC(X_train_20,X_test_20,y_train_20,y_test_20)\n",
    "print('Using 40% as test subset:')\n",
    "sRFC_y_predicted420 = sRFC(X_train_40,X_test_40,y_train_40,y_test_40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f40b2a8",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b88a5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sLR(X_train,X_test,y_train,y_test):\n",
    "    model = LogisticRegression(random_state=0).fit(X_train,y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    sLR_y_predicted = model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    print(classification_report(y_test, preds))\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, preds))\n",
    "    print(\"MCC =\",matthews_corrcoef(y_test, preds))\n",
    "    \n",
    "    return sLR_y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49ed00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Using 20% as test subset:')\n",
    "sLR_y_predicted_20 = sLR(X_train_20,X_test_20,y_train_20,y_test_20)\n",
    "print('Using 40% as test subset:')\n",
    "sLR_y_predicted_40 = sLR(X_train_40,X_test_40,y_train_40,y_test_40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4956197",
   "metadata": {},
   "source": [
    "## Features Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d95a1d",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination (RFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc872ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfe_xgb(X_train, y_train,X_test,learning_rate, n_estimators, max_depth,min_child_weight, gamma, subsample, colsample_bytree, simple):\n",
    "    min_features_to_select = 55\n",
    "    \n",
    "    #run RFE on current train subset\n",
    "    if simple:\n",
    "        clf = xgb.XGBClassifier(seed = 24, use_label_encoder =False)\n",
    "    else:\n",
    "        clf = xgb.XGBClassifier(learning_rate = learning_rate, n_estimators = int(n_estimators), max_depth = int(max_depth), \n",
    "                                min_child_weight = min_child_weight, gamma = gamma, subsample = subsample, \n",
    "                                colsample_bytree = colsample_bytree, seed = 24,eval_metric='mlogloss',use_label_encoder =False)\n",
    "    rfecv = RFECV(estimator=clf,min_features_to_select=min_features_to_select,step=3,n_jobs=-1,scoring=\"r2\",cv=5)\n",
    "    rfecv.fit(X_train, y_train)\n",
    "    \n",
    "    #keep selected features + check RFE accuracy scores during running\n",
    "    newX_train = X_train[X_train.columns[rfecv.support_]]\n",
    "    newX_test = X_test[X_test.columns[rfecv.support_]]\n",
    "    print('We kept',newX_train.shape[1],'features out of the',X_train.shape[1])\n",
    "    \n",
    "    return newX_train,newX_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67631c6e",
   "metadata": {},
   "source": [
    "### Test on single XGB classifier using RFE selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5e9b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test rfe then simple XGB classifier\n",
    "newX_train,newX_test = rfe_xgb(X_train_20, y_train_20,X_test_20,0,0,0,0,0,0,0,simple=True)\n",
    "RFE_XGB_y_predicted = XGB_class(newX_train,newX_test,y_train_20,y_test_20,0,0,0,0,0,0,0,simple=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be19de70",
   "metadata": {},
   "source": [
    "### Test on single SVM classifier using RFE selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e31359",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "min_features_to_select = 199\n",
    "\n",
    "#run RFE on current train subset\n",
    "clf = SVC(kernel='linear',probability=True) \n",
    "\n",
    "rfecv = RFECV(estimator=clf,min_features_to_select=min_features_to_select,step=3,n_jobs=-1,scoring=\"r2\",cv=5)\n",
    "rfecv.fit(X_train_40, y_train_40.values.ravel())\n",
    "\n",
    "#keep selected features + check RFE accuracy scores during running\n",
    "newX_train = X_train_40[X_train_40.columns[rfecv.support_]]\n",
    "newX_test = X_test_40[X_test_40.columns[rfecv.support_]]\n",
    "print('We kept',newX_train.shape[1],'features out of the',X_train_40.shape[1])\n",
    "  \n",
    "# fitting x samples and y classes \n",
    "clf.fit(newX_train, y_train_40) \n",
    "preds = clf.predict(newX_test)\n",
    "RFE_SVM_y_predicted = clf.predict_proba(newX_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test_40, preds))\n",
    "print(\"Accuracy:\", accuracy_score(y_test_40, preds))\n",
    "print(\"MCC =\",matthews_corrcoef(y_test_40, preds))\n",
    "print(\"AUC =\",metrics.roc_auc_score(y_test_40, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0f8f54",
   "metadata": {},
   "source": [
    "### Plot AUC-ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d6c1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc(y_tests,y_predicted,models_name,title):    \n",
    "    for i in range(len(y_predicted)):\n",
    "        fpr, tpr, _ = metrics.roc_curve(y_tests[i],  y_predicted[i])\n",
    "        #create ROC curve\n",
    "        plt.plot(fpr, tpr, linestyle='--')\n",
    "        \n",
    "    plt.legend(models_name)\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fb5606",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tests = np.array([y_test_20,y_test_40])\n",
    "y_preds = np.array([RFE_XGB_y_predicted,RFE_SVM_y_predicted])\n",
    "models_name=np.array(['RFE_XGBc 20%','RFE_SVM 40%'])\n",
    "title = 'ROC curve of stronger models using RFE'\n",
    "roc(y_tests,y_preds,models_name,title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b2b054",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tests = np.array([y_test_20,y_test_20,y_test_20,y_test_20,y_test_40,y_test_40,y_test_40,y_test_40])\n",
    "y_preds = np.array([sXGBc_y_predicted_20,sSVM_y_predicted_20,sRFC_y_predicted_20,sLR_y_predicted_20,sXGBc_y_predicted_40,sSVM_y_predicted_40,sRFC_y_predicted_40,sLR_y_predicted_40])\n",
    "models_name=np.array(['sXGBc 20%','SVM 20%','sRFC 20%','LR 20%','sXGBc 40%','SVM 40%','sRFC 40%','LR 40%'])\n",
    "title = 'ROC curve of simple models with split 80/20 and 60/40'\n",
    "roc(y_tests,y_preds,models_name,title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17e3d52",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning XGBoost classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43353abf",
   "metadata": {},
   "source": [
    "### Using Evolutionary Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c85ca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(723)\n",
    "np.random.seed(723)\n",
    "\n",
    "def initilialize_poplulation(numberOfParents):\n",
    "    learningRate = np.empty([numberOfParents, 1])\n",
    "    nEstimators = np.empty([numberOfParents, 1], dtype = np.uint8)\n",
    "    maxDepth = np.empty([numberOfParents, 1], dtype = np.uint8)\n",
    "    minChildWeight = np.empty([numberOfParents, 1])\n",
    "    gammaValue = np.empty([numberOfParents, 1])\n",
    "    subSample = np.empty([numberOfParents, 1])\n",
    "    colSampleByTree =  np.empty([numberOfParents, 1])\n",
    "\n",
    "    for i in range(numberOfParents):\n",
    "        learningRate[i] = round(random.uniform(0.01, 1), 2)\n",
    "        nEstimators[i] = random.randrange(10, 1500, step = 25)\n",
    "        maxDepth[i] = int(random.randrange(1, 10, step= 1))\n",
    "        minChildWeight[i] = round(random.uniform(0.01, 10.0), 2)\n",
    "        gammaValue[i] = round(random.uniform(0.01, 10.0), 2)\n",
    "        subSample[i] = round(random.uniform(0.01, 1.0), 2)\n",
    "        colSampleByTree[i] = round(random.uniform(0.01, 1.0), 2)\n",
    "    \n",
    "    population = np.concatenate((learningRate, nEstimators, maxDepth, minChildWeight, gammaValue, subSample, colSampleByTree), axis= 1)\n",
    "    return population\n",
    "\n",
    "   \n",
    "\n",
    "def fitness_accuracy_score(y_true, y_pred):\n",
    "    fitness = round((accuracy_score(y_true, y_pred)), 4)\n",
    "    return fitness\n",
    "\n",
    "def train_population(population, dMatrixTrain, dMatrixtest, y_test):\n",
    "    aScore = []\n",
    "    for i in range(population.shape[0]):\n",
    "        param = { 'objective':'binary:logistic',\n",
    "              'learning_rate': population[i][0],\n",
    "              'n_estimators': population[i][1], \n",
    "              'max_depth': int(population[i][2]), \n",
    "              'min_child_weight': population[i][3],\n",
    "              'gamma': population[i][4], \n",
    "              'subsample': population[i][5],\n",
    "              'colsample_bytree': population[i][6],\n",
    "              'seed': 24}\n",
    "        num_round = 100\n",
    "        xgbT = xgb.train(param, dMatrixTrain, num_round)\n",
    "        preds = xgbT.predict(dMatrixtest)\n",
    "        preds = preds>0.5\n",
    "        aScore.append(fitness_accuracy_score(y_test, preds))\n",
    "    return aScore\n",
    "\n",
    "\n",
    "\n",
    "def new_parents_selection(population, fitness, numParents):\n",
    "    selectedParents = np.empty((numParents, population.shape[1])) \n",
    "    \n",
    "    for parentId in range(numParents):\n",
    "        bestFitnessId = np.where(fitness == np.max(fitness))\n",
    "        bestFitnessId  = bestFitnessId[0][0]\n",
    "        selectedParents[parentId, :] = population[bestFitnessId, :]\n",
    "        fitness[bestFitnessId] = -1 \n",
    "    return selectedParents\n",
    "        \n",
    "\n",
    "def crossover_uniform(parents, childrenSize):\n",
    "    \n",
    "    crossoverPointIndex = np.arange(0, np.uint8(childrenSize[1]), 1, dtype= np.uint8)\n",
    "    crossoverPointIndex1 = np.random.randint(0, np.uint8(childrenSize[1]), np.uint8(childrenSize[1]/2)) \n",
    "    crossoverPointIndex2 = np.array(list(set(crossoverPointIndex) - set(crossoverPointIndex1))) \n",
    "    \n",
    "    children = np.empty(childrenSize)\n",
    "    \n",
    "    \n",
    "    for i in range(childrenSize[0]):\n",
    "        \n",
    "        parent1_index = i%parents.shape[0]\n",
    "        parent2_index = (i+1)%parents.shape[0]\n",
    "        children[i, crossoverPointIndex1] = parents[parent1_index, crossoverPointIndex1]\n",
    "        children[i, crossoverPointIndex2] = parents[parent2_index, crossoverPointIndex2]\n",
    "    return children\n",
    "    \n",
    "\n",
    "\n",
    "def mutation(crossover, numberOfParameters):\n",
    "\n",
    "    minMaxValue = np.zeros((numberOfParameters, 2))\n",
    "    \n",
    "    minMaxValue[0:] = [0.01, 1.0] \n",
    "    minMaxValue[1, :] = [10, 2000] \n",
    "    minMaxValue[2, :] = [1, 15] \n",
    "    minMaxValue[3, :] = [0, 10.0] \n",
    "    minMaxValue[4, :] = [0.01, 10.0] \n",
    "    minMaxValue[5, :] = [0.01, 1.0] \n",
    "    minMaxValue[6, :] = [0.01, 1.0] \n",
    " \n",
    "    \n",
    "    mutationValue = 0\n",
    "    parameterSelect = np.random.randint(0, 7, 1)\n",
    "    print(parameterSelect)\n",
    "    if parameterSelect == 0: \n",
    "        mutationValue = round(np.random.uniform(-0.5, 0.5), 2)\n",
    "    if parameterSelect == 1: \n",
    "        mutationValue = np.random.randint(-200, 200, 1)\n",
    "    if parameterSelect == 2:\n",
    "        mutationValue = np.random.randint(-5, 5, 1)\n",
    "    if parameterSelect == 3: \n",
    "        mutationValue = round(np.random.uniform(5, 5), 2)\n",
    "    if parameterSelect == 4: \n",
    "        mutationValue = round(np.random.uniform(-2, 2), 2)\n",
    "    if parameterSelect == 5: \n",
    "        mutationValue = round(np.random.uniform(-0.5, 0.5), 2)\n",
    "    if parameterSelect == 6: \n",
    "        mutationValue = round(np.random.uniform(-0.5, 0.5), 2)\n",
    "  \n",
    "    \n",
    "    for idx in range(crossover.shape[0]):\n",
    "        crossover[idx, parameterSelect] = crossover[idx, parameterSelect] + mutationValue\n",
    "        if(crossover[idx, parameterSelect] > minMaxValue[parameterSelect, 1]):\n",
    "            crossover[idx, parameterSelect] = minMaxValue[parameterSelect, 1]\n",
    "        if(crossover[idx, parameterSelect] < minMaxValue[parameterSelect, 0]):\n",
    "            crossover[idx, parameterSelect] = minMaxValue[parameterSelect, 0]    \n",
    "    return crossover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0cfc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp_param_ev_algo(X_train, X_test, y_train, y_test):\n",
    "    xgDMatrix = xgb.DMatrix(X_train, y_train) \n",
    "    xgbDMatrixTest = xgb.DMatrix(X_test, y_test)\n",
    "\n",
    "    numberOfParents = 100 \n",
    "    numberOfParentsMating = int(numberOfParents/2)\n",
    "    numberOfParameters = 7 \n",
    "    numberOfGenerations = 5\n",
    "\n",
    "    populationSize = (numberOfParents, numberOfParameters)\n",
    "    population = initilialize_poplulation(numberOfParents)\n",
    "\n",
    "    fitnessHistory = np.empty([numberOfGenerations+1, numberOfParents])\n",
    "    populationHistory = np.empty([(numberOfGenerations+1)*numberOfParents, numberOfParameters])\n",
    "    populationHistory[0:numberOfParents, :] = population\n",
    "\n",
    "    for generation in range(numberOfGenerations):\n",
    "        print(\"This is number %s generation\" % (generation))\n",
    "\n",
    "        fitnessValue = train_population(population=population, dMatrixTrain=xgDMatrix, dMatrixtest=xgbDMatrixTest, y_test=y_test)\n",
    "        fitnessHistory[generation, :] = fitnessValue\n",
    "\n",
    "        print('Best Accuracy score in the this iteration = {}'.format(np.max(fitnessHistory[generation, :])))\n",
    "\n",
    "        parents = new_parents_selection(population=population, fitness=fitnessValue, numParents=numberOfParentsMating)\n",
    "        children = crossover_uniform(parents=parents, childrenSize=(populationSize[0] - parents.shape[0], numberOfParameters))\n",
    "        children_mutated = mutation(children, numberOfParameters)\n",
    "\n",
    "        population[0:parents.shape[0], :] = parents \n",
    "        population[parents.shape[0]:, :] = children_mutated \n",
    "\n",
    "        populationHistory[(generation+1)*numberOfParents : (generation+1)*numberOfParents+ numberOfParents , :] = population \n",
    "\n",
    "    fitness = train_population(population=population, dMatrixTrain=xgDMatrix, dMatrixtest=xgbDMatrixTest, y_test=y_test)\n",
    "    fitnessHistory[generation+1, :] = fitness\n",
    "\n",
    "    bestFitnessIndex = np.where(fitness == np.max(fitness))[0][0]\n",
    "\n",
    "    print(\"Best fitness is =\", fitness[bestFitnessIndex])\n",
    "\n",
    "\n",
    "    print(\"Best parameters are:\")\n",
    "    print('learning_rate', population[bestFitnessIndex][0])\n",
    "    print('n_estimators', population[bestFitnessIndex][1])\n",
    "    print('max_depth', int(population[bestFitnessIndex][2])) \n",
    "    print('min_child_weight', population[bestFitnessIndex][3])\n",
    "    print('gamma', population[bestFitnessIndex][4])\n",
    "    print('subsample', population[bestFitnessIndex][5])\n",
    "    print('colsample_bytree', population[bestFitnessIndex][6])\n",
    "    \n",
    "    return population[bestFitnessIndex][0],population[bestFitnessIndex][1],population[bestFitnessIndex][2],population[bestFitnessIndex][3],population[bestFitnessIndex][4],population[bestFitnessIndex][5],population[bestFitnessIndex][6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aa068d",
   "metadata": {},
   "source": [
    "### Order 1) First RFE, then tuning hyperparameters on selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eacc83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test tuning XGB hyperparameters with selected features from RFE\n",
    "newX_train,newX_test = rfe_xgb(X_train_40, y_train_40,X_test_40,0,0,0,0,0,0,0,simple=True)\n",
    "learning_rate, n_estimators, max_depth, min_child_weight,gamma, subsample,colsample_bytree = hyp_param_ev_algo(newX_train, newX_test, y_train_40, y_test_40)\n",
    "XGB_class(newX_train,newX_test,y_train_40,y_test_40,learning_rate, n_estimators, max_depth, min_child_weight,gamma, subsample,colsample_bytree,simple=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1077158",
   "metadata": {},
   "source": [
    "### Order 2) First tuning hyperparameters, then RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c758b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate, n_estimators, max_depth, min_child_weight,gamma, subsample,colsample_bytree = hyp_param_ev_algo(X_train_40, X_test_40, y_train_40, y_test_40)\n",
    "newX_train,newX_test = rfe_xgb(X_train_40, y_train_40,X_test_40,learning_rate, n_estimators, max_depth, min_child_weight,gamma, subsample,colsample_bytree,simple=True)\n",
    "XGB_class(newX_train,newX_test,y_train_40,y_test_40,learning_rate, n_estimators, max_depth, min_child_weight,gamma, subsample,colsample_bytree,simple=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4781f3fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
