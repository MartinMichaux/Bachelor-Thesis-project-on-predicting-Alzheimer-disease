{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "70e18535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import poisson\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import xgboost as xgb\n",
    "xgb.set_config(verbosity=0)\n",
    "from sklearn.feature_selection import RFECV\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4e3475",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8c8f6110",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset\\CSF_Proteomics_ADNI.csv\")\n",
    "\n",
    "#replace categorical feature with index labeling\n",
    "df['binary_class'].replace({'stable':0,'decliner':1},inplace=True)\n",
    "df['three_class'].replace({'slowDecline':0,'rapidDecline':1,'stable':2},inplace=True)\n",
    "\n",
    "#differentiate other categorical features from the numerical ones\n",
    "pheno = df.loc[:,'RID':'VISCODE']\n",
    "data = df.loc[:,'A1AT.AVLTIDEK':'VTDB.VPTADLEDVLPLAEDITNILSK']\n",
    "\n",
    "## Normalization\n",
    "\n",
    "#normally test function -> check if each column is normally distr\n",
    "def norm_test(data):\n",
    "    alpha = 1e-3\n",
    "    k2, p = stats.normaltest(data)\n",
    "    count=0\n",
    "    for i in p:\n",
    "        if i > alpha:  # null hypothesis: x comes from a normal distribution\n",
    "            count+=1\n",
    "    print('There are ',count,'normally distributed features out of',data.shape[1])\n",
    "\n",
    "#QUANTILE NORMALIZATION\n",
    "def quantile_normalize(df):\n",
    "    df_sorted = pd.DataFrame(np.sort(df.values,axis=0),index=df.index,columns=df.columns)\n",
    "    df_mean = df_sorted.mean(axis=1)\n",
    "    df_mean.index = np.arange(1, len(df_mean) + 1)\n",
    "    df_qn =df.rank(method=\"min\").stack().astype(int).map(df_mean).unstack()\n",
    "    return df_qn\n",
    "\n",
    "# compute quantile normalized data\n",
    "df_qn=quantile_normalize(data)\n",
    "data = df_qn\n",
    "\n",
    "### Box-cox transformation\n",
    "df_bc = pd.DataFrame().reindex_like(data)\n",
    "\n",
    "for col in df_qn:\n",
    "    df_bc[col],_ = stats.boxcox(df_qn[col])\n",
    "data = df_qn\n",
    "\n",
    "## Split dataset\n",
    "\n",
    "X = data\n",
    "y = df[\"binary_class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c797dc",
   "metadata": {},
   "source": [
    "## Test models using with PCA as feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "28eebb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note that we now have 55 columns instead of 320\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Make an instance of the Model keeping 95% of the features variance is retained\n",
    "pca = PCA(.95)\n",
    "\n",
    "principalComponents = pca.fit_transform(X)\n",
    "principalDf = pd.DataFrame(data = principalComponents)\n",
    "print('Note that we now have',principalDf.shape[1],'columns instead of',X.shape[1])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(principalDf, y, test_size = 0.20, random_state = 97)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af91e1e5",
   "metadata": {},
   "source": [
    "### Single xgb classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1a3282d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGB_class(X_train, X_test, y_train, y_test,learning_rate, n_estimators, max_depth,min_child_weight, gamma, subsample, colsample_bytree, simple):\n",
    "    \n",
    "    if simple:\n",
    "        clf = xgb.XGBClassifier(seed = 24, use_label_encoder =False)\n",
    "    else:\n",
    "        clf = xgb.XGBClassifier(learning_rate = learning_rate, n_estimators = int(n_estimators), max_depth = int(max_depth), \n",
    "                                min_child_weight = min_child_weight, gamma = gamma, subsample = subsample, \n",
    "                                colsample_bytree = colsample_bytree, seed = 24,eval_metric='mlogloss',use_label_encoder =False)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_predicted = clf.predict(X_test)\n",
    "\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_predicted))\n",
    "    print(classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5bb59e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.48148148148148145\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.27      0.30        11\n",
      "           1       0.56      0.62      0.59        16\n",
      "\n",
      "    accuracy                           0.48        27\n",
      "   macro avg       0.44      0.45      0.44        27\n",
      "weighted avg       0.47      0.48      0.47        27\n",
      "\n"
     ]
    }
   ],
   "source": [
    "XGB_class(X_train,X_test,y_train,y_test,0,0,0,0,0,0,0,simple=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9601563c",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "88e6f378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6296296296296297\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.64      0.58        11\n",
      "           1       0.71      0.62      0.67        16\n",
      "\n",
      "    accuracy                           0.63        27\n",
      "   macro avg       0.63      0.63      0.62        27\n",
      "weighted avg       0.64      0.63      0.63        27\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(kernel='linear') \n",
    "  \n",
    "# fitting x samples and y classes \n",
    "clf.fit(X_train, y_train) \n",
    "y_predicted = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_predicted))\n",
    "print(classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e5ad9a",
   "metadata": {},
   "source": [
    "### Single Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a4edcd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6296296296296297\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.18      0.29        11\n",
      "           1       0.62      0.94      0.75        16\n",
      "\n",
      "    accuracy                           0.63        27\n",
      "   macro avg       0.65      0.56      0.52        27\n",
      "weighted avg       0.64      0.63      0.56        27\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate random forest algorithm for classification\n",
    "# define the model\n",
    "model = RandomForestClassifier().fit(X_train,y_train)\n",
    "y_predicted = model.predict(X_test)\n",
    "# report performance\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_predicted))\n",
    "print(classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc17aa7b",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4ec8a707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5925925925925926\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.45      0.48        11\n",
      "           1       0.65      0.69      0.67        16\n",
      "\n",
      "    accuracy                           0.59        27\n",
      "   macro avg       0.57      0.57      0.57        27\n",
      "weighted avg       0.59      0.59      0.59        27\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "y_predicted = model.predict(X_test)\n",
    "# report performance\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_predicted))\n",
    "print(classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "86eaec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameter tuning XGBoost classifier\n",
    "\n",
    "random.seed(723)\n",
    "np.random.seed(723)\n",
    "\n",
    "def initilialize_poplulation(numberOfParents):\n",
    "    learningRate = np.empty([numberOfParents, 1])\n",
    "    nEstimators = np.empty([numberOfParents, 1], dtype = np.uint8)\n",
    "    maxDepth = np.empty([numberOfParents, 1], dtype = np.uint8)\n",
    "    minChildWeight = np.empty([numberOfParents, 1])\n",
    "    gammaValue = np.empty([numberOfParents, 1])\n",
    "    subSample = np.empty([numberOfParents, 1])\n",
    "    colSampleByTree =  np.empty([numberOfParents, 1])\n",
    "\n",
    "    for i in range(numberOfParents):\n",
    "        learningRate[i] = round(random.uniform(0.01, 1), 2)\n",
    "        nEstimators[i] = random.randrange(10, 1500, step = 25)\n",
    "        maxDepth[i] = int(random.randrange(1, 10, step= 1))\n",
    "        minChildWeight[i] = round(random.uniform(0.01, 10.0), 2)\n",
    "        gammaValue[i] = round(random.uniform(0.01, 10.0), 2)\n",
    "        subSample[i] = round(random.uniform(0.01, 1.0), 2)\n",
    "        colSampleByTree[i] = round(random.uniform(0.01, 1.0), 2)\n",
    "    \n",
    "    population = np.concatenate((learningRate, nEstimators, maxDepth, minChildWeight, gammaValue, subSample, colSampleByTree), axis= 1)\n",
    "    return population\n",
    "\n",
    "   \n",
    "\n",
    "def fitness_accuracy_score(y_true, y_pred):\n",
    "    fitness = round((accuracy_score(y_true, y_pred)), 4)\n",
    "    return fitness\n",
    "\n",
    "def train_population(population, dMatrixTrain, dMatrixtest, y_test):\n",
    "    aScore = []\n",
    "    for i in range(population.shape[0]):\n",
    "        param = { 'objective':'binary:logistic',\n",
    "              'learning_rate': population[i][0],\n",
    "              'n_estimators': population[i][1], \n",
    "              'max_depth': int(population[i][2]), \n",
    "              'min_child_weight': population[i][3],\n",
    "              'gamma': population[i][4], \n",
    "              'subsample': population[i][5],\n",
    "              'colsample_bytree': population[i][6],\n",
    "              'seed': 24}\n",
    "        num_round = 100\n",
    "        xgbT = xgb.train(param, dMatrixTrain, num_round)\n",
    "        preds = xgbT.predict(dMatrixtest)\n",
    "        preds = preds>0.5\n",
    "        aScore.append(fitness_accuracy_score(y_test, preds))\n",
    "    return aScore\n",
    "\n",
    "\n",
    "\n",
    "def new_parents_selection(population, fitness, numParents):\n",
    "    selectedParents = np.empty((numParents, population.shape[1])) \n",
    "    \n",
    "    for parentId in range(numParents):\n",
    "        bestFitnessId = np.where(fitness == np.max(fitness))\n",
    "        bestFitnessId  = bestFitnessId[0][0]\n",
    "        selectedParents[parentId, :] = population[bestFitnessId, :]\n",
    "        fitness[bestFitnessId] = -1 \n",
    "    return selectedParents\n",
    "        \n",
    "\n",
    "def crossover_uniform(parents, childrenSize):\n",
    "    \n",
    "    crossoverPointIndex = np.arange(0, np.uint8(childrenSize[1]), 1, dtype= np.uint8)\n",
    "    crossoverPointIndex1 = np.random.randint(0, np.uint8(childrenSize[1]), np.uint8(childrenSize[1]/2)) \n",
    "    crossoverPointIndex2 = np.array(list(set(crossoverPointIndex) - set(crossoverPointIndex1))) \n",
    "    \n",
    "    children = np.empty(childrenSize)\n",
    "    \n",
    "    \n",
    "    for i in range(childrenSize[0]):\n",
    "        \n",
    "        parent1_index = i%parents.shape[0]\n",
    "        parent2_index = (i+1)%parents.shape[0]\n",
    "        children[i, crossoverPointIndex1] = parents[parent1_index, crossoverPointIndex1]\n",
    "        children[i, crossoverPointIndex2] = parents[parent2_index, crossoverPointIndex2]\n",
    "    return children\n",
    "    \n",
    "\n",
    "\n",
    "def mutation(crossover, numberOfParameters):\n",
    "\n",
    "    minMaxValue = np.zeros((numberOfParameters, 2))\n",
    "    \n",
    "    minMaxValue[0:] = [0.01, 1.0] \n",
    "    minMaxValue[1, :] = [10, 2000] \n",
    "    minMaxValue[2, :] = [1, 15] \n",
    "    minMaxValue[3, :] = [0, 10.0] \n",
    "    minMaxValue[4, :] = [0.01, 10.0] \n",
    "    minMaxValue[5, :] = [0.01, 1.0] \n",
    "    minMaxValue[6, :] = [0.01, 1.0] \n",
    " \n",
    "    \n",
    "    mutationValue = 0\n",
    "    parameterSelect = np.random.randint(0, 7, 1)\n",
    "    print(parameterSelect)\n",
    "    if parameterSelect == 0: \n",
    "        mutationValue = round(np.random.uniform(-0.5, 0.5), 2)\n",
    "    if parameterSelect == 1: \n",
    "        mutationValue = np.random.randint(-200, 200, 1)\n",
    "    if parameterSelect == 2:\n",
    "        mutationValue = np.random.randint(-5, 5, 1)\n",
    "    if parameterSelect == 3: \n",
    "        mutationValue = round(np.random.uniform(5, 5), 2)\n",
    "    if parameterSelect == 4: \n",
    "        mutationValue = round(np.random.uniform(-2, 2), 2)\n",
    "    if parameterSelect == 5: \n",
    "        mutationValue = round(np.random.uniform(-0.5, 0.5), 2)\n",
    "    if parameterSelect == 6: \n",
    "        mutationValue = round(np.random.uniform(-0.5, 0.5), 2)\n",
    "  \n",
    "    \n",
    "    for idx in range(crossover.shape[0]):\n",
    "        crossover[idx, parameterSelect] = crossover[idx, parameterSelect] + mutationValue\n",
    "        if(crossover[idx, parameterSelect] > minMaxValue[parameterSelect, 1]):\n",
    "            crossover[idx, parameterSelect] = minMaxValue[parameterSelect, 1]\n",
    "        if(crossover[idx, parameterSelect] < minMaxValue[parameterSelect, 0]):\n",
    "            crossover[idx, parameterSelect] = minMaxValue[parameterSelect, 0]    \n",
    "    return crossover\n",
    "\n",
    "def hyp_param_ev_algo(X_train, X_test, y_train, y_test):\n",
    "    xgDMatrix = xgb.DMatrix(X_train, y_train) \n",
    "    xgbDMatrixTest = xgb.DMatrix(X_test, y_test)\n",
    "\n",
    "    numberOfParents = 100 \n",
    "    numberOfParentsMating = int(numberOfParents/2)\n",
    "    numberOfParameters = 7 \n",
    "    numberOfGenerations = 50\n",
    "\n",
    "    populationSize = (numberOfParents, numberOfParameters)\n",
    "    population = initilialize_poplulation(numberOfParents)\n",
    "\n",
    "    fitnessHistory = np.empty([numberOfGenerations+1, numberOfParents])\n",
    "    populationHistory = np.empty([(numberOfGenerations+1)*numberOfParents, numberOfParameters])\n",
    "    populationHistory[0:numberOfParents, :] = population\n",
    "\n",
    "    for generation in range(numberOfGenerations):\n",
    "        print(\"This is number %s generation\" % (generation))\n",
    "\n",
    "        fitnessValue = train_population(population=population, dMatrixTrain=xgDMatrix, dMatrixtest=xgbDMatrixTest, y_test=y_test)\n",
    "        fitnessHistory[generation, :] = fitnessValue\n",
    "\n",
    "        print('Best Accuracy score in the this iteration = {}'.format(np.max(fitnessHistory[generation, :])))\n",
    "\n",
    "        parents = new_parents_selection(population=population, fitness=fitnessValue, numParents=numberOfParentsMating)\n",
    "        children = crossover_uniform(parents=parents, childrenSize=(populationSize[0] - parents.shape[0], numberOfParameters))\n",
    "        children_mutated = mutation(children, numberOfParameters)\n",
    "\n",
    "        population[0:parents.shape[0], :] = parents \n",
    "        population[parents.shape[0]:, :] = children_mutated \n",
    "\n",
    "        populationHistory[(generation+1)*numberOfParents : (generation+1)*numberOfParents+ numberOfParents , :] = population \n",
    "\n",
    "    fitness = train_population(population=population, dMatrixTrain=xgDMatrix, dMatrixtest=xgbDMatrixTest, y_test=y_test)\n",
    "    fitnessHistory[generation+1, :] = fitness\n",
    "\n",
    "    bestFitnessIndex = np.where(fitness == np.max(fitness))[0][0]\n",
    "\n",
    "    print(\"Best fitness is =\", fitness[bestFitnessIndex])\n",
    "\n",
    "\n",
    "    print(\"Best parameters are:\")\n",
    "    print('learning_rate', population[bestFitnessIndex][0])\n",
    "    print('n_estimators', population[bestFitnessIndex][1])\n",
    "    print('max_depth', int(population[bestFitnessIndex][2])) \n",
    "    print('min_child_weight', population[bestFitnessIndex][3])\n",
    "    print('gamma', population[bestFitnessIndex][4])\n",
    "    print('subsample', population[bestFitnessIndex][5])\n",
    "    print('colsample_bytree', population[bestFitnessIndex][6])\n",
    "    \n",
    "    return population[bestFitnessIndex][0],population[bestFitnessIndex][1],population[bestFitnessIndex][2],population[bestFitnessIndex][3],population[bestFitnessIndex][4],population[bestFitnessIndex][5],population[bestFitnessIndex][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2eb7e113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is number 0 generation\n",
      "Best Accuracy score in the this iteration = 0.7778\n",
      "[2]\n",
      "This is number 1 generation\n",
      "Best Accuracy score in the this iteration = 0.7778\n",
      "[2]\n",
      "This is number 2 generation\n",
      "Best Accuracy score in the this iteration = 0.7778\n",
      "[1]\n",
      "This is number 3 generation\n",
      "Best Accuracy score in the this iteration = 0.7778\n",
      "[2]\n",
      "This is number 4 generation\n",
      "Best Accuracy score in the this iteration = 0.8148\n",
      "[4]\n",
      "This is number 5 generation\n",
      "Best Accuracy score in the this iteration = 0.8148\n",
      "[1]\n",
      "This is number 6 generation\n",
      "Best Accuracy score in the this iteration = 0.8148\n",
      "[2]\n",
      "This is number 7 generation\n",
      "Best Accuracy score in the this iteration = 0.8148\n",
      "[4]\n",
      "This is number 8 generation\n",
      "Best Accuracy score in the this iteration = 0.8148\n",
      "[1]\n",
      "This is number 9 generation\n",
      "Best Accuracy score in the this iteration = 0.8148\n",
      "[3]\n",
      "This is number 10 generation\n",
      "Best Accuracy score in the this iteration = 0.8148\n",
      "[0]\n",
      "This is number 11 generation\n",
      "Best Accuracy score in the this iteration = 0.8148\n",
      "[0]\n",
      "This is number 12 generation\n",
      "Best Accuracy score in the this iteration = 0.8148\n",
      "[3]\n",
      "This is number 13 generation\n",
      "Best Accuracy score in the this iteration = 0.8148\n",
      "[6]\n",
      "This is number 14 generation\n",
      "Best Accuracy score in the this iteration = 0.8148\n",
      "[4]\n",
      "This is number 15 generation\n",
      "Best Accuracy score in the this iteration = 0.8148\n",
      "[3]\n",
      "This is number 16 generation\n",
      "Best Accuracy score in the this iteration = 0.8148\n",
      "[3]\n",
      "This is number 17 generation\n",
      "Best Accuracy score in the this iteration = 0.8148\n",
      "[4]\n",
      "This is number 18 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[4]\n",
      "This is number 19 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[3]\n",
      "This is number 20 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[2]\n",
      "This is number 21 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[3]\n",
      "This is number 22 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[0]\n",
      "This is number 23 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[0]\n",
      "This is number 24 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[5]\n",
      "This is number 25 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[5]\n",
      "This is number 26 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[5]\n",
      "This is number 27 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[2]\n",
      "This is number 28 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[3]\n",
      "This is number 29 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[0]\n",
      "This is number 30 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[1]\n",
      "This is number 31 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[3]\n",
      "This is number 32 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[1]\n",
      "This is number 33 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[3]\n",
      "This is number 34 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[0]\n",
      "This is number 35 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[6]\n",
      "This is number 36 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[2]\n",
      "This is number 37 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[1]\n",
      "This is number 38 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[5]\n",
      "This is number 39 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[4]\n",
      "This is number 40 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[2]\n",
      "This is number 41 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[5]\n",
      "This is number 42 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[6]\n",
      "This is number 43 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[4]\n",
      "This is number 44 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[4]\n",
      "This is number 45 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[2]\n",
      "This is number 46 generation\n",
      "Best Accuracy score in the this iteration = 0.8519\n",
      "[2]\n",
      "This is number 47 generation\n",
      "Best Accuracy score in the this iteration = 0.9259\n",
      "[6]\n",
      "This is number 48 generation\n",
      "Best Accuracy score in the this iteration = 0.9259\n",
      "[4]\n",
      "This is number 49 generation\n",
      "Best Accuracy score in the this iteration = 0.9259\n",
      "[0]\n",
      "Best fitness is = 0.9259\n",
      "Best parameters are:\n",
      "learning_rate 0.76\n",
      "n_estimators 10.0\n",
      "max_depth 11\n",
      "min_child_weight 5.44\n",
      "gamma 3.86\n",
      "subsample 0.61\n",
      "colsample_bytree 0.79\n",
      "Accuracy: 0.5555555555555556\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.36      0.40        11\n",
      "           1       0.61      0.69      0.65        16\n",
      "\n",
      "    accuracy                           0.56        27\n",
      "   macro avg       0.53      0.53      0.52        27\n",
      "weighted avg       0.54      0.56      0.55        27\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test tuning XGB hyperparameters with selected features from RFE\n",
    "learning_rate, n_estimators, max_depth, min_child_weight,gamma, subsample,colsample_bytree = hyp_param_ev_algo(X_train, X_test, y_train, y_test)\n",
    "XGB_class(X_train,X_test,y_train,y_test,learning_rate, n_estimators, max_depth, min_child_weight,gamma, subsample,colsample_bytree,simple=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f6b08641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is number 0 generation\n",
      "Best Accuracy score in the this iteration = 0.7407\n",
      "[0]\n",
      "This is number 1 generation\n",
      "Best Accuracy score in the this iteration = 0.7407\n",
      "[3]\n",
      "This is number 2 generation\n",
      "Best Accuracy score in the this iteration = 0.7407\n",
      "[2]\n",
      "This is number 3 generation\n",
      "Best Accuracy score in the this iteration = 0.7407\n",
      "[3]\n",
      "This is number 4 generation\n",
      "Best Accuracy score in the this iteration = 0.7407\n",
      "[5]\n",
      "This is number 5 generation\n",
      "Best Accuracy score in the this iteration = 0.7407\n",
      "[0]\n",
      "This is number 6 generation\n",
      "Best Accuracy score in the this iteration = 0.7778\n",
      "[0]\n",
      "This is number 7 generation\n",
      "Best Accuracy score in the this iteration = 0.8148\n",
      "[6]\n",
      "This is number 8 generation\n",
      "Best Accuracy score in the this iteration = 0.8148\n",
      "[1]\n",
      "This is number 9 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[0]\n",
      "This is number 10 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[4]\n",
      "This is number 11 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[6]\n",
      "This is number 12 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[1]\n",
      "This is number 13 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[5]\n",
      "This is number 14 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[6]\n",
      "This is number 15 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[1]\n",
      "This is number 16 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[3]\n",
      "This is number 17 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[2]\n",
      "This is number 18 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[5]\n",
      "This is number 19 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[4]\n",
      "This is number 20 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[2]\n",
      "This is number 21 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[0]\n",
      "This is number 22 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[6]\n",
      "This is number 23 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[5]\n",
      "This is number 24 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[3]\n",
      "This is number 25 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[0]\n",
      "This is number 26 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[6]\n",
      "This is number 27 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[6]\n",
      "This is number 28 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[0]\n",
      "This is number 29 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[0]\n",
      "This is number 30 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[6]\n",
      "This is number 31 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[1]\n",
      "This is number 32 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[1]\n",
      "This is number 33 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[0]\n",
      "This is number 34 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[5]\n",
      "This is number 35 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[0]\n",
      "This is number 36 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[6]\n",
      "This is number 37 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[1]\n",
      "This is number 38 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[4]\n",
      "This is number 39 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[4]\n",
      "This is number 40 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[4]\n",
      "This is number 41 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[6]\n",
      "This is number 42 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[2]\n",
      "This is number 43 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[3]\n",
      "This is number 44 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[4]\n",
      "This is number 45 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[1]\n",
      "This is number 46 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[6]\n",
      "This is number 47 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[6]\n",
      "This is number 48 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[1]\n",
      "This is number 49 generation\n",
      "Best Accuracy score in the this iteration = 0.8889\n",
      "[5]\n",
      "Best fitness is = 0.8889\n",
      "Best parameters are:\n",
      "learning_rate 0.6200000000000001\n",
      "n_estimators 246.0\n",
      "max_depth 2\n",
      "min_child_weight 6.39\n",
      "gamma 5.3\n",
      "subsample 0.56\n",
      "colsample_bytree 0.49\n",
      "Accuracy: 0.8888888888888888\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.82      0.86        11\n",
      "           1       0.88      0.94      0.91        16\n",
      "\n",
      "    accuracy                           0.89        27\n",
      "   macro avg       0.89      0.88      0.88        27\n",
      "weighted avg       0.89      0.89      0.89        27\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test RFE with tuning XGB hyperparameters\n",
    "learning_rate, n_estimators, max_depth, min_child_weight,gamma, subsample,colsample_bytree = hyp_param_ev_algo(X_train, X_test, y_train, y_test)\n",
    "XGB_class(X_train,X_test,y_train,y_test,learning_rate, n_estimators, max_depth, min_child_weight,gamma, subsample,colsample_bytree,simple=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da27c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
